## About Us

We are the **I**ntelligent **M**ultimedia **C**ontent **C**omputing (**IMCC**) Lab members at **U**niversity of **S**cience and **T**echnology of **C**hina (USTC). 

This paper reading report about Computer Vision, with special emphasis on **Fine-grained Recognition**, **Weakly-supervised Learning**, **Causal Inference**, **Imperfect Data Learning** and relevant topics. We aim to provide an opportunity for students, researchers and faculties to discuss and keep eyes on the current progress in Computer Vision, and to learn how to do high-quality research.

For any interest in our report or our lab, please contact Doctor [Chuanbin Liu](http://home.ustc.edu.cn/~lcb592/).

## Format

| Date       | Presenter                                        | Venue        | Paper Title                                                                                                                                                              | Slides                        |
|:----------:|:------------------------------------------------:|:------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------:|
| 2020.04.12 | [Chuanbin Liu](http://home.ustc.edu.cn/~lcb592/) | NeurIPS 2019 | [This Looks Like That: Deep Learning for Interpretable Image Recognition](https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html) | [Slides](Slides/lcb_20200412.pdf) |

- **Date**: The date of the report. Please arrange in **reverse chronological order**.
- **Presenter**: The presenter of the report. You can also provide your **personal link**.
- **Venue**: The Venue of the report.
- **Paper Title**: Provide the title and link of this paper.
- **Slides**: Please convert your **.ppt** document to **.pdf** document with name **Presenter_Date** (e.g. **lcb_20200412**), and keep it within **5M**. As you know, GitHub limits the size of files and the storage of repositories. Also please upload your **.ppt** document to **our tencent document**.

## Schedule

| Date       | Presenter                                        | Venue        | Paper Title                                                                                                                                                                                                                  | Slides                            |
|:----------:|:------------------------------------------------:|:------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------:|
|2025.06.09 | Yunning Cao | arxiv | [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/pdf/2505.22019)|[Slides](Slides/cyn_20250609_vrag.pdf)|
|2025.04.16 | Peicheng Zhou | - | MULTIMODAL SITUATIONAL SAFETY | [Slides](Slides/zpc_20250604.pdf) |
|2025.04.16 | Bowei Pu | - | MultiModal PRM | [Slides](Slides/pbw_20250414.pdf) |
|2025.04.03 | Yixuan Zhang | - | Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models | [Slides](Slides/zyx_20250403.pdf) |
|2025.02.25 | Yinglu Li | - | Parametric Retrieval Augmented Generation | [Slides](Slides/liyl20250225.pdf) |
|2025.01.14 | Luohao Lin | - | AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting| [Slides](Slides/llh_20250114.pdf)|
|2024.12.24 | Borui Ding | - | Chain-of-Thought Reasoning without Prompting| [Slides](Slides/dbr_20241224.pdf)|
|2024.12.17 | Yiwei Sun | - | A Few Papers about Safety Alignment for MLLM| [Slides](Slides/syw_20241217.pdf)|
|2024.11.19 | Zhiying Lu | - | Where Can We Mix? From Atom to Cosmic| [Slides](Slides/lzy_20241119.pdf)|
|2024.10.10 | Yunning Cao | CVPR2024 | [Compositional Chain-of-Thought Prompting for Large Multimodal Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Mitra_Compositional_Chain-of-Thought_Prompting_for_Large_Multimodal_Models_CVPR_2024_paper.pdf) | [Slides](Slides/cyn_20241010_sgg.pdf)|
|2024.08.28 | Yixuan Zhang | Arxiv | xGen-MM (BLIP-3): A Family of Open Large Multimodal Models | [Slides](Slides/zyx_20240827.pdf)|
|2024.08.21 | Yifan Gao | Arxiv | ControlNeXt: Powerful and Efficient Control for Image and Video Generation | [Slides](Slides/gyf_20240723.pdf)|
|2024.07.16 | Zhiying Lu | Arxiv | Cambrian-1:A Fully Open, Vision-CentricExploration of Multimodal LLMs | [Slides](Slides/lzy_20240716.pdf)|
|2024.07.09 | Yunning Cao | CVPR2024 | [VISTA-LLAMA: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_VISTA-LLAMA_Reducing_Hallucination_in_Video_Language_Models_via_Equal_Distance_CVPR_2024_paper.pdf) | [Slides](Slides/cyn_20240709vistallama.pdf)|
|2024.07.02 | Yinglu Li | Arxiv | AnyTrans: Translate AnyText in the Image with Large Scale Models | [Slides](Slides/liyl_20240702.pdf)|
|2024.06.25 | Bowei Pu | CVPR2024 | Two papers about Video CLIP and Long Video MLLM | [Slides](Slides/pbw_20240625.pdf)|
|2024.06.11 | Yifan Gao | Arxiv | Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering | [Slides](Slides/gyf_20240625.pdf)|
|2024.06.04 | Peicheng Zhou | CVPR2024 | Exploration of the reasons for Limiting MLLM performance | [Slides](Slides/zhoupc_20240604.pdf)|
|2024.05.28 | TianLe Hu | CVPR2024 | [Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs](https://arxiv.org/abs/2310.00582) | [Slides](Slides/hutl_20240528.pdf)|
|2024.05.21 | Yiwei Sun | - | Two papers about Video LLM| [Slides](Slides/syw_20240521.pdf)|
|2024.05.14 | Yixuan Zhang | - | Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding| [Slides](Slides/zyx_20240514.pdf)|
|2024.04.15 | Borui Ding | - | masked images are counterfactual samples for robust fine-tuning | [Slides](Slides/dbr_20240509.pdf)|
|2024.04.08 | Yifan Gao | - | A Suvery on Text Image Generation | [Slides](Slides/gyf_20230408.pdf)|
|2024.03.26 | Zhiying Lu | - | Pretrained ViT as Vision Encoder | [Slides](Slides/lzy_20240326.pdf)|
|2024.03.19 | Yunning Cao | CVPR2024 | [Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs](https://arxiv.org/pdf/2401.06209.pdf) | [Slides](Slides/cyn_20240319mof.pdf)|
|2024.03.12 | Yiwei Sun | - | A Survey on MLLM: IT, ICL & CoT | [Slides](Slides/syw_20240312.pdf)|
|2024.03.05 | TianLe Hu | CVPR2024 | [Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy Tradeoff for Out-of-Distribution Few-shot Learning](https://arxiv.org/abs/2311.13612) | [Slides](Slides/hutl_20240305.pdf)|
|2023.11.21 | Zhiying Lu | arxiv | [Intializing Models with Larger Ones](https://arxiv.org/abs/2311.18823) | [Slides](Slides/lzy_20231205.pdf)|
|2023.11.07 | Tianle Hu | ICCV2023 | [Waffling around for Performance: Visual Classification with Random Words and Broad Concepts](https://arxiv.org/abs/2306.07282) | [Slides](Slides/hutl_20231107.pdf)|
|2023.11.01 | Yifan Gao | - | Image-based Visual Try-on | [Slides](Slides/gyf_20231101.pdf)|
|2023.10.10 | Yiwei Sun | - | A Survey on Compositional Understanding  | [Slides](Slides/syw_20231010.pdf)|
|2023.09.26 | Zhiying Lu | - | I can't believe there is no training!  | [Slides](Slides/lzy_20230926.pdf)|
|2023.09.12 | Yunning Cao | ICCV2023 | [I can’t believe there’s no images! Learning Visual Tasks Using Only Language Supervision](https://arxiv.org/pdf/2211.09778.pdf) | [Slides](Slides/cyn_20230912MG.pdf)|
|2023.07.25 | Jingyuan Xu | CVPR2022 | [Grounded_Language-Image_Pre-Training](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.pdf) | [Slides](Slides/xujy20230725-GLIP.pdf)|
|2023.07.11 | Yiwei Sun | CVPR2023 | [Extracting Class Activation Maps from Non-Discriminative Features as well](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Extracting_Class_Activation_Maps_From_Non-Discriminative_Features_As_Well_CVPR_2023_paper.pdf) | [Slides](Slides/syw_20230711.pdf)|
|2023.07.04 | Tinle Hu | CVPR2023 | SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer | [Slides](Slides/hutl_20230704.pdf)|
|2023.06.26 | Yixuan Zhang | ICLR2023 | Context Autoencoder for Self-Supervised Representation Learning | [Slides](Slides/zyx_20230626.pdf)|
|2023.06.26 | Tianhao Qi | - | A Survey on Controllable Text-to-Image Diffusion Models  | [Slides](Slides/qth_20230626.pdf)|
|2023.06.19 | Borui Ding | NIPS2023 | Vision Transformer Adapter For Dense Predictions | [Slides](Slides/dbr_20230620.pdf)|
|2023.06.12 | Yifan Gao | - | A Survey on Vision Prompt Tuning Learning  | [Slides](Slides/gyf_20230612.pdf)|
|2023.06.08 | Pandeng Li | - | A Survey on Multi-modal Pretraining  | [Slides](Slides/lpd_20230608Pretraining.pdf)|
|2023.06.08 | Yunning Cao | - | A Survey on Visual Tuning  | [Slides](Slides/cyn_20230608VisualTuning.pdf)|
|2023.06.05 | Zhiying Lu | arxiv | VanillaNet: the Power of Minimalism in Deep Learning  | [Slides](Slides/lzy_20230605.pdf)|
|2023.05.29 | Yunning Cao | CVPR2023 | Texts as Images in Prompt Tuning for Multi-Label Image Recognition  | [Slides](Slides/cyn_20230529TextasImage.pdf)|
|2023.05.23 | Jingyuan Xu | CVPR2023 | Aligning Bag of Regions for Open-Vocabulary Object Detection | [Slides](Slides/xjy_20230523ovod.pdf)|
|2023.05.15 | Fanchao Lin | arxiv | A demo survey on recent fundamental models and applications | [Slides](Slides/lfc_20230515.pdf)|
|2023.05.08 | Yifan Gao | - | A Survey on Fine-Grained Self-Supervised Learning | [Slides](Slides/gyf_20230508.pdf)|
|2023.04.27 | Zhiying Lu | CVPR2023 | Non-Global Attention Mechanisms In Vision Transformers | [Slides](Slides/lzy_20230427.pdf)|
|2023.04.10 | Yunning Cao | arxiv | [Segment Anything](https://arxiv.org/pdf/2304.02643.pdf) | [Slides](Slides/cyn_20230411SAM.pdf)|
|2023.03.27|Yiwei Sun|-|How to help your ViT learn the inductive bias?|[Slides](Slides/syw_20230327.pdf)|
|2023.03.20|Yunyan Yan|-|Regression: Representation Space|[Slides](Slides/yyy_20230320.pdf)|
|2023.03.13 | Jingyuan Xu | ICLR 2023 | [F-VLM: OPEN-VOCABULARY OBJECT DETECTION UPON FROZEN VISION AND LANGUAGE MODELS](https://arxiv.org/abs/2209.15639) | [Slides](Slides/xujy_20230313FVLM.pdf)|
|2023.03.06 | Yixuan Zhang | ECCV 2022 | [Adaptive Token Sampling For Efficient Vision Transformers](https://arxiv.org/abs/2111.15667) | [Slides](Slides/zyx_20230306.pdf)|
|2023.02.27 | Fanchao Lin | NIPS 2022 | [Training language models to follow instructions with human feedback](https://openreview.net/pdf?id=TG8KACxEON) | [Slides](Slides/lfc_20230227.pdf)|
|2023.02.20 | Yifan Gao | NIPS 2022 | [ConvMAE: Masked Convolution Meets Masked Autoencoders](https://arxiv.org/abs/2205.03892) | [Slides](Slides/gyf_20230220.pdf)
|2023.02.06 | Yunyan Yan | CVPR 2022 | [A Re-Balancing Strategy for Class-Imbalanced Classification Based on Instance Difficulty](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_A_Re-Balancing_Strategy_for_Class-Imbalanced_Classification_Based_on_Instance_Difficulty_CVPR_2022_paper.pdf) | [Slides](Slides/yyy_20230206.pdf)|
|2023.01.03 | Yunning Cao | ICLR 2023 | [Image as Set of Points](https://openreview.net/pdf?id=awnvqZja69) | [Slides](Slides/cyn_20230102.pdf)|
|2022.12.19|Yiwei Sun|-|A Survey on FGVC|[Slides](Slides/syw_20221219.pdf)|
| 2022.12.14 | Fanchao Lin| CVPR 2022 | [Recurrent Dynamic Embedding for Video Object Segmentation](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Recurrent_Dynamic_Embedding_for_Video_Object_Segmentation_CVPR_2022_paper.html) | [Slides](Slides/lfc_20221214.pdf)     |
| 2022.12.05 | Yunyan Yan                                      | AAAI 2019    | [Gradient Harmonized Single-Stage Detector](https://ojs.aaai.org/index.php/AAAI/article/view/4877) | [Slides](Slides/yyy_20221205.pdf) |
| 2022.11.28 | Yunning Cao                                      | CVPR 2022    | [Fine-Grained Object Classification via Self-Supervised Pose Alignment](https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Fine-Grained_Object_Classification_via_Self-Supervised_Pose_Alignment_CVPR_2022_paper.html) | [Slides](Slides/cyn_20221127.pdf) |
| 2022.11.28 | [Zhiying Lu](https://github.com/ArieSeirack)     | ECCV 2022    | [TokenMix: Rethinking Image Mixing for Data Augmentation in Vision Transformers](https://arxiv.org/abs/2207.08409) | [Slides](Slides/lzy_20220905.pdf) |
| 2020.04.12 | [Chuanbin Liu](http://home.ustc.edu.cn/~lcb592/) | NeurIPS 2019 | [This Looks Like That: Deep Learning for Interpretable Image Recognition](https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html)                                                     | [Slides](Slides/lcb_20200412.pdf)     |

